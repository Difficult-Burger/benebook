# Benebook Project LLM Feature Pipeline Analysis Report

## 1. Overview

Benebook is a book donation and points exchange platform that integrates intelligent assistant functionality based on Large Language Models (LLM). This intelligent assistant can handle various user queries, including data queries and casual conversations. This report will analyze in detail the complete processing pipeline of the LLM functionality in the Benebook project.

## 2. Technology Stack

- **Frontend**: HTML, CSS, JavaScript, Bootstrap, EJS Template Engine
- **Backend**: Node.js, Express.js
- **Database**: MySQL
- **LLM Service**: DeepSeek API

## 3. LLM Feature Pipeline Flow

### 3.1 User Interface Layer

Users interact with the intelligent assistant through the chat sidebar on the web interface:

1. Users enter questions or requests in the chat input box
2. Frontend JavaScript code captures user input and sends it to the backend via API
3. The chat interface displays the user's message and shows a "Thinking..." loading status
4. After receiving the backend response, the AI assistant's reply is displayed

Interface features:
- Responsive design, adapts to different device sizes
- Chat messages use different styles to distinguish between users and assistants
- Automatic scrolling to the latest message
- Input box disabled to prevent duplicate submissions

### 3.2 Request Processing Layer

The backend processes user requests through Express routes:

1. The `/api/chat/message` endpoint receives POST requests containing user messages
2. Adds user messages to the chat history
3. Performs intent classification: determines whether the user request is a data query or casual chat
4. Builds different prompts based on intent
5. Calls external LLM service (DeepSeek API) to get responses
6. Processes the responses returned by the LLM and performs further operations based on request type
7. Returns the processed results to the frontend

Other auxiliary interfaces:
- `/api/chat/history`: Retrieves chat history
- `/api/chat/clear`: Clears chat history

### 3.3 Intent Classification Layer

The system analyzes user input through the `isDataRequest` function to determine whether it is a data query or ordinary conversation:

1. First checks if it matches explicit casual chat patterns (e.g., greetings, thanks, etc.)
2. Then checks if it matches explicit data query patterns (e.g., "query", "display" plus data objects)
3. If no explicit match, checks whether it simultaneously contains data operation words (e.g., "query", "display") and data object words (e.g., "books", "price")
4. Returns a boolean value based on these analyses, indicating whether it is a data request

This classification method combines rule pattern matching and keyword recognition to improve the accuracy of intent recognition.

### 3.4 Prompt Construction Layer

Based on the intent classification results, the system constructs different types of prompts:

**Data Query Prompt**:
- Built through the `buildDataRequestPrompt` function
- Contains complete database structure descriptions (table structures and fields)
- Requires the LLM to generate accurate SQL query statements
- Explicitly instructs to return only SQL statements, without any other explanations

**Casual Chat Prompt**:
- Built through the `buildChatPrompt` function
- Contains previous conversation history
- Sets role and behavioral standards for the AI assistant
- Encourages friendly and professional answers

### 3.5 LLM Invocation Layer

The system calls the LLM service provided by DeepSeek through API:

1. Configures API key and URL endpoint
2. Builds request body conforming to DeepSeek API specifications
3. Specifies the model to use (deepseek-chat)
4. Sends HTTP POST request
5. Receives and parses JSON response
6. Extracts the text content generated by the model

### 3.6 Result Processing Layer

The system processes the results returned by the LLM according to different types of requests:

**Data Query Requests**:
1. Extracts and cleans SQL statements (removes code block markers)
2. Executes SQL queries to get results
3. Formats user-friendly responses based on query type and results
   - Supports different types of queries: grouped queries, aggregate queries, book queries, donor queries, etc.
   - Special formatting for different types of data (e.g., adding currency symbols to prices, formatting dates)
4. Adds formatted responses to chat history
5. Returns complete information (response content, chat history, SQL query, and results)

**Casual Chat Requests**:
1. Uses the LLM's reply directly as the response
2. Adds to chat history
3. Returns to the frontend

### 3.7 Error Handling Layer

The system implements a multi-level error handling mechanism:

1. **SQL Execution Errors**: Captures and formats error messages, returns friendly prompts
2. **API Call Errors**: Handles network or service errors, provides appropriate feedback
3. **Frontend Error Handling**: Displays error messages at the UI layer and restores the operation interface

## 4. Key Functional Features

1. **Hybrid Query Processing**: Simultaneously supports database queries and natural language dialogue
2. **Context Maintenance**: Maintains chat history records, supports continuous dialogue
3. **Intelligent Intent Recognition**: Uses rules and pattern matching to determine user intent
4. **Custom Result Formatting**: Adjusts display format according to query type and results
5. **Real-time Interaction**: Processes requests asynchronously, maintains UI responsiveness


## 5. Conclusion

The LLM feature pipeline of the Benebook project implements a complete process from user input to AI response. By combining rule judgments and large language models, it achieves an intelligent assistant function that can both answer general questions and execute data queries. The system demonstrates how to efficiently integrate and utilize large language models in practical applications to provide users with natural, smooth interactive experiences. 